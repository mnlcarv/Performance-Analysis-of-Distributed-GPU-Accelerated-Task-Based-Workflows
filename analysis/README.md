# How to run experiments

The experiments were executed on the [Minotauro cluster](https://bsc.es/supportkc/docs/Minotauro/overview/) with the distributed system [COMPSs](https://compss-doc.readthedocs.io/en/stable/index.html) using the version with bindings for Python (PyCOMPSs).

COMPSs applications are executed with the ```runcompss``` command, as shown in the example below:
```
runcompss my_compss_app.py
```
In a cluster, COMPSs applications are executed via batch jobs. A job is submitted using the ```enqueue_compss``` command. For more details, please check the basic queue commands in COMPSs [here](https://compss-doc.readthedocs.io/en/stable/Sections/03_Execution_Environments/03_Deployments/01_Master_worker/02_Supercomputers/03_Minotauro.html?highlight=supercomputer).

To extract data (de-)serialization times, it is necessary to activate tracing in COMPSs by including the extra argument ```--tracing=true``` to the execution command, as follows:
```
runcompss --tracing=true run_kmeans.py
```
The execution traces generated by COMPSs with [Extrae](https://tools.bsc.es/extrae) can be visualized using [Paraver](https://www.bsc.es/discover-bsc/organisation/scientific-structure/performance-tools/paraver). More information about traces in COMPSs and how to visualize them are available at the links below:

[COMPSs Applications Tracing](https://compss-doc.readthedocs.io/en/stable/Sections/05_Tools/03_Tracing/01_Apps_tracing.html)

[Trace Visualization in Paraver](https://compss-doc.readthedocs.io/en/stable/Sections/05_Tools/03_Tracing/02_Visualization.html?highlight=paraver)


## Steps to run experiments in a compatible system

1. Setup the system to run experiments. The dependencies and code references are detailed below:

### Dependencies
- COMPSs >= v3.0
- Python >= v3.7
- dislib >= v0.6.4
- NumPy >= v1.18.1
- CuPy >= v10

### Code references
[K-means](https://github.com/mnlcarv/Performance-Analysis-of-Distributed-GPU-Accelerated-Task-Based-Workflows/blob/main/analysis/algorithms/dislib/cluster/kmeans/base.py): dislib implementation of the K-means algorithm.

[Matmul](https://github.com/mnlcarv/Performance-Analysis-of-Distributed-GPU-Accelerated-Task-Based-Workflows/blob/main/analysis/algorithms/dislib/data/array.py): dislib implementation of the Matmul algorithm.

[Matmul FMA](https://compss-doc.readthedocs.io/en/stable/Sections/07_Sample_Applications/02_Python/04_Matmul.html?highlight=matmul): alternative algorithm, not belonging to the dislib library.

[dislib](https://github.com/bsc-wdc/dislib/tree/gpu-support): reference for dislib GPU support branch repository.

2. Run [run_generate_parameters.py](https://github.com/mnlcarv/Performance-Analysis-of-Distributed-GPU-Accelerated-Task-Based-Workflows/blob/main/analysis/run_generate_parameters.py) to export the execution parameters to the file [tb_parameters.csv](https://github.com/mnlcarv/Performance-Analysis-of-Distributed-GPU-Accelerated-Task-Based-Workflows/blob/main/analysis/parameters/tb_parameters.csv). Choose a desired combination of parameters by filtering the query available on this script

3. Run [run_experiments.py](https://github.com/mnlcarv/Performance-Analysis-of-Distributed-GPU-Accelerated-Task-Based-Workflows/blob/main/analysis/run_experiments.py) to run experiments using the parameters collected in the previous step. After running this script the raw logs with all the monitored metrics<sup>†</sup> will be stored in the file [tb_experiments_raw.csv](https://github.com/mnlcarv/Performance-Analysis-of-Distributed-GPU-Accelerated-Task-Based-Workflows/blob/main/analysis/results/tb_experiments_raw.csv).

Note: because parallel applications run asynchronously compared to sequential applications<sup>††</sup>, measuring the execution times in such applications requires synchronization barriers to ensure that all operations in a task are finished. For this reason, we prepared three execution modes for each processor type (CPU or GPU) controlled by the flag ```id_device``` as follows<sup>†††</sup>:

- ```id_device = 1``` or ```id_device = 2```: execution with COMPSs synchronizations to extract total execution times (end-to-end execution) for CPUs and GPUs, respectively.

- ```id_device = 3``` or ```id_device = 4```: execution to extract task user code execution times for CPUs and GPUs (CUDA event synchronization is required for GPUs), respectively.

- ```id_device = 5 or id_device = 6```: execution with COMPSs synchronizations to extract parallel tasks execution times for CPUs and GPUs, respectively.

<sup>†</sup>Data deserialization and serialization times are obtained with the profiling tool [Paraver](https://www.bsc.es/discover-bsc/organisation/scientific-structure/performance-tools/paraver).

<sup>††</sup>More information about synchronization in COMPSs and CuPY are available in the links below:

[COMPSs synchronization](https://compss-doc.readthedocs.io/en/stable/Sections/02_App_Development/02_Python/01_2_Synchronization/01_API.html)

[CuPy synchronization](https://docs.cupy.dev/en/stable/user_guide/performance.html)

<sup>†††</sup>Except for the execution mode ```id_device = 4```, which requires [CUDA events](https://docs.cupy.dev/en/stable/user_guide/performance.html), all the execution times are extracted using [Python's performance counter](https://docs.python.org/3/library/time.html)

4. Run [run_insert_results.py](https://github.com/mnlcarv/Performance-Analysis-of-Distributed-GPU-Accelerated-Task-Based-Workflows/blob/main/analysis/run_insert_results.py) to insert the monitored metrics obtained in the previous step into the fact table ```EXPERIMENT_RAW``` table


## Methodology to extend our analysis to other setups

1. Define a execution setting to study
    - Execution parameters considering the task algorithm, dataset, resources, and system employed
    - Execution performance metrics related to:
        - Execution times
            - Task user code metrics: CPU-GPU communication and serial, parallel, and user code execution times 
            - Data movement overheads: data serialization and deserialization times
            - Task level metrics: parallel task execution time
        - Any other metric of interest besides time (e.g. power consumption, throughput, etc)

2. Use our proposed star-schema model to store parameters in the dimension tables and the monitored metrics in the fact table

3. Select subsets of execution parameters from the dimension tables to vary in the experiments and monitor how their variation affects the performance metrics

4. Identify the subset of execution parameters that have the highest impact in the performance metrics (i.e., that causes the largest variations in the metrics) and consider them as the key factors in your setup