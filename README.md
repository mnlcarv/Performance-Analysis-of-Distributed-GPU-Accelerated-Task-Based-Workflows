# Introduction
This is the public repository of the paper "Performance Analysis of Distributed GPU-Accelerated Task-Based Workflows", which includes the artifacts required to reproduce our results. 
In this study we conduct such a multi-disciplinary study involving HPC, data science and data management. Such interdisciplinary study is not trivial and it is a time-consuming process which requires identifying and setting up the right experimental environment, identifying the right knobs to tune and the right metrics to analyze, and running a plethora of time-consuming micro-experiments. We believe reporting and opening the results of such a systematic analysis offers to the data community a useful guideline toward accelerating distributed applications and avoiding pitfalls. 
In the following we detail the experiments, datasets, and source code used in our empirical analysis. We also provide a methodology to extend the rationale behind our analysis to other setups.

## How to run the experiments
The experiments included in the paper were executed with the distributed system [COMPSs](https://compss-doc.readthedocs.io/en/stable/index.html) using the version with bindings for Python (PyCOMPSs). The sample algorithms (```run_kmeans.py```, ```run_matmul.py```, and ```run_matmul_fma.py```) are executed with the ```runcompss``` command, as shown in the example below:
```
runcompss run_kmeans.py
```
After running the example, the raw execution time logs are saved to a CSV file located at ```/experiments/results/tb_experiments_raw.csv```. Note that our scripts can be easily adapted to extract the execution times from other algorithms.

### Observations
1. In a cluster, the algorithms are executed via batch jobs. A job can be submitted using the ```enqueue_compss``` command. For more details, please check the basic queue commands in COMPSs [here](https://compss-doc.readthedocs.io/en/stable/Sections/03_Execution_Environments/03_Deployments/01_Master_worker/02_Supercomputers/03_Minotauro.html?highlight=supercomputer).
2. To extract data (de-)serialization times, it is necessary to activate tracing in COMPSs by including the extra argument ```--tracing=true``` to the execution command, as follows:
```
runcompss --tracing=true run_kmeans.py
```
The execution traces generated by COMPSs with [Extrae](https://tools.bsc.es/extrae) can be visualized using [Paraver](https://www.bsc.es/discover-bsc/organisation/scientific-structure/performance-tools/paraver). More information about traces in COMPSs and how to visualize them are available at the links below:

[COMPSs Applications Tracing](https://compss-doc.readthedocs.io/en/stable/Sections/05_Tools/03_Tracing/01_Apps_tracing.html)

[Trace Visualization in Paraver](https://compss-doc.readthedocs.io/en/stable/Sections/05_Tools/03_Tracing/02_Visualization.html?highlight=paraver)

## Dependencies
- COMPSs >= v3.0
- Python >= v3.7
- dislib >= v0.6.4
- NumPy >= v1.18.1
- CuPy >= v10

## Code references
[Matmul](https://github.com/mnlcarv/Performance-Analysis-of-Distributed-GPU-Accelerated-Task-Based-Workflows/blob/main/dislib/data/array.py): dislib implementation of the Matmul algorithm.

[K-means](https://github.com/mnlcarv/Performance-Analysis-of-Distributed-GPU-Accelerated-Task-Based-Workflows/blob/main/dislib/cluster/kmeans/base.py): dislib implementation of the K-means algorithm.

[Matmul FMA](https://compss-doc.readthedocs.io/en/stable/Sections/07_Sample_Applications/02_Python/04_Matmul.html?highlight=matmul): alternative algorithm, not belonging to the dislib library.

[dislib](https://github.com/bsc-wdc/dislib/tree/gpu-support): reference for dislib GPU support branch repository.


# Reproducibility of our results

## Database of experiments
We stored the main findings from our paper in a database. Specifically, we created a star-like schema with dimensions (i.e., the parameters of our experiments) and one fact table (the results of each experiment). The dimensions represent the dimensions, factors and parameters identified in the paper. Note that each row in the fact table is a unique combination of parameters identifying the experiment conducted. We used a relational database (Postgres) to store the execution parameters and results with all the metrics monitored in our experiments (except data deserialization and serialization times, which were obtained using [Paraver](https://www.bsc.es/discover-bsc/organisation/scientific-structure/performance-tools/paraver) and stored in CSV files). We stored the parameters data in eight dimension tables (```DEVICE```, ```ALGORITHM```, ```FUNCTION```, ```CONFIGURATION```, ```RESOURCE```, ```DATASET```, ```PARAMETER_TYPE```, and ```PARAMETER```) and the result data in a fact table (```EXPERIMENT_RAW```). Each row in the ```PARAMETER``` table represents a unique combination of execution parameters, which are identified in the ```EXPERIMENT_RAW``` table by the column ```ID_PARAMETER```.

The tables from the database were exported to this repository as CSV files and they are available in the path [/reproducibility/database/](https://github.com/mnlcarv/Performance-Analysis-of-Distributed-GPU-Accelerated-Task-Based-Workflows/blob/main/reproducibility/database/). The database dump is fragmented to each algorithm in different folders: [/reproducibility/database/schema_kmeans/](https://github.com/mnlcarv/Performance-Analysis-of-Distributed-GPU-Accelerated-Task-Based-Workflows/blob/main/reproducibility/database/schema_kmeans/) for K-means, [/reproducibility/database/schema_matmul/](https://github.com/mnlcarv/Performance-Analysis-of-Distributed-GPU-Accelerated-Task-Based-Workflows/blob/main/reproducibility/database/schema_matmul/) for Matmul, and [/reproducibility/database/schema_matmul_fma/](https://github.com/mnlcarv/Performance-Analysis-of-Distributed-GPU-Accelerated-Task-Based-Workflows/blob/main/reproducibility/database/schema_matmul_fma/) for Matmul FMA.

Data deserialization and serialization times (columns 'Deserializing object' and 'Serializing object' in [Paraver's histogram](https://compss-doc.readthedocs.io/en/stable/Sections/05_Tools/03_Tracing/04_Analysis.html)) were stored in different CSV files and they are available at [/reproducibility/paraver/](https://github.com/mnlcarv/Performance-Analysis-of-Distributed-GPU-Accelerated-Task-Based-Workflows/blob/main/reproducibility/paraver/). Use the columns ```Algorithm```, ```Device```, ```Data Set Size```, and ```Block Size MB``` to link the results from Paraver to the database. 

## Queries to reproduce the charts in the experiments
The queries used to extract the data plotted in each chart in our experiments are available at [/reproducibility/queries_charts/](https://github.com/mnlcarv/Performance-Analysis-of-Distributed-GPU-Accelerated-Task-Based-Workflows/blob/main/reproducibility/queries_charts/). To reproduce our results, load the tables (available in the schema folders in the path [/reproducibility/database/](https://github.com/mnlcarv/Performance-Analysis-of-Distributed-GPU-Accelerated-Task-Based-Workflows/blob/main/reproducibility/database/)) into a database and run our queries. Data obtained from Paraver (available at [/reproducibility/paraver/](https://github.com/mnlcarv/Performance-Analysis-of-Distributed-GPU-Accelerated-Task-Based-Workflows/blob/main/reproducibility/paraver/)) must be merged manually following the instructions explained previously to link the results to the database.


# Replicability of our methodology

## Extending our analysis to other setups
We ran our experiments using the resources available at the [Minotauro](https://bsc.es/supportkc/docs/Minotauro/overview/) cluster using [COMPSs](https://compss-doc.readthedocs.io/en/stable/index.html) as the distributed system. However, it is possible to extend our methodology to other setups as follows:

**1. System and execution parameters:** Select a distributed system with support for GPU acceleration and identify the key execution parameters. Our study surfaced relevant parameters in COMPSs that are applicable to other popular distributed systems, such as [Apache Spark](https://www.usenix.org/conference/nsdi12/technical-sessions/presentation/zaharia), but eventually specific parameters (e.g., [input data format (CSV or Parquet) in Spark](https://ieeexplore.ieee.org/abstract/document/9438792/)) might also have a considerable impact in performance, demanding extra fine-tuning for a task-based analysis.

**2. Algorithm and task user code:** Select an algorithm to study and identify the GPU-accelerated task(s) available in it. Make sure that the algorithm has two versions of the same task(s) (i.e. a version for CPUs and another for GPUs) to perform comparisons between CPUs and GPUs. Adapt the task user code to extract the evaluated metrics (see Section 4.2 in the paper)<sup>†</sup>. Because parallel applications run asynchronously compared to sequential applications<sup>††</sup>, measuring the execution times in such applications requires synchronization barriers to ensure that all operations in a task are finished. For this reason, we prepared three execution modes (see how to run the sample algorithms ```run_kmeans.py```, ```run_matmul.py```, and ```run_matmul_fma.py```) for each processor type (CPU or GPU) controlled by the flag ```id_device``` as follows<sup>†††</sup>:

- ```id_device = 1``` or ```id_device = 2```: execution with COMPSs synchronizations to extract total execution times (end-to-end execution) for CPUs and GPUs, respectively.

- ```id_device = 3``` or ```id_device = 4```: execution to extract task user code execution times for CPUs and GPUs (CUDA event synchronization is required for GPUs), respectively.

- ```id_device = 5 or id_device = 6```: execution with COMPSs synchronizations to extract parallel tasks execution times for CPUs and GPUs, respectively.

**3. Execution of experiments:** Once the execution parameters are identified, define a proper set of execution parameters to study in the experiments by combining algorithm, dataset, resource, and system parameters (see Table 1 in the paper). Use the dimension tables ```DEVICE```, ```ALGORITHM```, ```FUNCTION```, ```CONFIGURATION```, ```RESOURCE```, ```DATASET```, ```PARAMETER_TYPE```, and ```PARAMETER``` (if necessary, adapt them by adding or removing columns according to the new setup) to store the set of execution parameters. For each parameter, run the algorithm using the execution modes from the previous step. The raw execution time logs are saved to a CSV file located at ```/experiments/results/tb_experiments_raw.csv```. Run the script [/reproducibility/scripts/python/insert_results.py](https://github.com/mnlcarv/Performance-Analysis-of-Distributed-GPU-Accelerated-Task-Based-Workflows/blob/main/reproducibility/scripts/python/insert_results.py) to insert the results into the ```EXPERIMENT_RAW``` table using the column ```ID_PARAMETER``` to link the results to the each execution parameter in the ```PARAMETER``` table:
```
python /reproducibility/scripts/python/insert_results.py
```
We encourage users to use the same database schema design to store their results. The SQL ```CREATE``` scripts for the tables as well as their documentation are available at [/reproducibility/scripts/postgres/](https://github.com/mnlcarv/Performance-Analysis-of-Distributed-GPU-Accelerated-Task-Based-Workflows/blob/main/reproducibility/scripts/postgres/).


<sup>†</sup>Data deserialization and serialization times can be obtained with third party profiling tools (e.g. [Paraver](https://www.bsc.es/discover-bsc/organisation/scientific-structure/performance-tools/paraver)).

<sup>††</sup>More information about synchronization in COMPSs and CuPY are available in the links below:

[COMPSs synchronization](https://compss-doc.readthedocs.io/en/stable/Sections/02_App_Development/02_Python/01_2_Synchronization/01_API.html)

[CuPy synchronization](https://docs.cupy.dev/en/stable/user_guide/performance.html)

<sup>†††</sup>Except for the execution mode ```id_device = 4```, which requires [CUDA events](https://docs.cupy.dev/en/stable/user_guide/performance.html), all the execution times are extracted using [Python's performance counter](https://docs.python.org/3/library/time.html).


# Discussion about the generalizability and supplementary experiments
We now provide details about additional experiments we provided related to the generalizability of our method. Specifically, we report about supplementary experiments about larger datasets, skewed and spart datasets and an additional algorithm to the two used in the paper.

The supplementary experiments were conducted using the same analysis method as in the experiments reported in the paper. The execution parameters and results obtained are reported at [/experiments/results/supplementary_experiments/Report_Supplementary_Experiments.pdf](https://github.com/mnlcarv/Performance-Analysis-of-Distributed-GPU-Accelerated-Task-Based-Workflows/blob/main/experiments/results/supplementary_experiments/Report_Supplementary_Experiments.pdf). In particular, we perfomed the following experiments:

**i. Larger datasets:** We executed K-means with a 100 GB dataset and Matmul with a 32 GB dataset. The results revealed similar trends observed in the experiments from Section 5 in the paper with respect to the metrics measured. However, as expected, the limited GPU memory (12 GB) prevented a full comparison between the results obtained in the new dataset sizes with the dataset sizes presented in the paper.

**ii. Skewed and sparse datasets:** For K-means, we generated a skewed dataset by adapting the uniform distribution of the NumPy [*random.random* routine](https://numpy.org/doc/stable/reference/random/generated/numpy.random.random.html) to move 50% of the elements to certain regions of the distribution forcing groups of elements in the dataset. For Matmul, we generated a sparse matrix. In both cases (i.e., for K-means with skewed data and Matmul with sparse data), we compared the results to the ones obtained using the uniform distribution with dense data and no relevant changes between the executions with different datasets were observed. This happened because there is no special treatment of skewed/sparse data in the algorithms and CPUs and GPUs processed such data the same way as a uniformly distributed dense data.

**iii. Different algorithms:** To show how our results also apply to different algorithms we tested the Fused Multiply Add matrix multiplication ([Matmul FMA](https://compss-doc.readthedocs.io/en/stable/Sections/07_Sample_Applications/02_Python/04_Matmul.html?highlight=matmul)), which merges both multiplication and addition operations into the same task, thus showing different task dependencies and data access patterns. Our results revealed the same trends presented in the paper (see Section 5 in the paper) for the tested algorithms from dislib with respect to the user code speedup, parallel fraction, and CPU-GPU communication times.
